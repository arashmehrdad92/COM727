{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJX33wxwzDN9",
        "outputId": "a9f53a24-3f67-4553-d5c4-57ffd4aaa7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available and will be used for training!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.0722 - loss: 3.4106\n",
            "Epoch 2/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3186 - loss: 2.5453\n",
            "Epoch 3/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6341 - loss: 1.5438\n",
            "Epoch 4/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7497 - loss: 0.9743\n",
            "Epoch 5/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8135 - loss: 0.6668\n",
            "Epoch 6/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8609 - loss: 0.5442\n",
            "Epoch 7/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8751 - loss: 0.4962\n",
            "Epoch 8/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8832 - loss: 0.3980\n",
            "Epoch 9/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8841 - loss: 0.3878\n",
            "Epoch 10/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8887 - loss: 0.3793\n",
            "Epoch 11/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9162 - loss: 0.3473\n",
            "Epoch 12/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9367 - loss: 0.2692\n",
            "Epoch 13/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9210 - loss: 0.2591\n",
            "Epoch 14/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9320 - loss: 0.2197\n",
            "Epoch 15/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9283 - loss: 0.2281\n",
            "Epoch 16/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9485 - loss: 0.2028\n",
            "Epoch 17/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9419 - loss: 0.2056\n",
            "Epoch 18/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9288 - loss: 0.2085\n",
            "Epoch 19/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1847\n",
            "Epoch 20/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9613 - loss: 0.1338\n",
            "Epoch 21/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9393 - loss: 0.1823\n",
            "Epoch 22/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9338 - loss: 0.2085\n",
            "Epoch 23/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9451 - loss: 0.1657\n",
            "Epoch 24/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9560 - loss: 0.1660\n",
            "Epoch 25/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9503 - loss: 0.1646\n",
            "Epoch 26/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9481 - loss: 0.1777\n",
            "Epoch 27/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9637 - loss: 0.1299\n",
            "Epoch 28/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9573 - loss: 0.1386\n",
            "Epoch 29/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9477 - loss: 0.1633\n",
            "Epoch 30/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9592 - loss: 0.1215\n",
            "Epoch 31/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9677 - loss: 0.1036\n",
            "Epoch 32/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9564 - loss: 0.1319\n",
            "Epoch 33/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9548 - loss: 0.1380\n",
            "Epoch 34/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9661 - loss: 0.1192\n",
            "Epoch 35/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9509 - loss: 0.1318\n",
            "Epoch 36/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9804 - loss: 0.0718\n",
            "Epoch 37/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9702 - loss: 0.1140\n",
            "Epoch 38/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9622 - loss: 0.1325\n",
            "Epoch 39/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9545 - loss: 0.1316\n",
            "Epoch 40/40\n",
            "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9589 - loss: 0.1234\n",
            "Model saved as 'chatbot_model.keras'\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Manually specify the NLTK data directory\n",
        "nltk_dir = \"/content/nltk_data\"\n",
        "os.makedirs(nltk_dir, exist_ok=True)\n",
        "nltk.data.path.append(nltk_dir)\n",
        "\n",
        "# Download necessary NLTK resources with force=True\n",
        "nltk.download('punkt', download_dir=nltk_dir, force=True)\n",
        "nltk.download('wordnet', download_dir=nltk_dir, force=True)\n",
        "nltk.download('stopwords', download_dir=nltk_dir, force=True)\n",
        "# Download 'punkt_tab' data\n",
        "nltk.download('punkt_tab', download_dir=nltk_dir, force=True) # This line downloads the required 'punkt_tab' data\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Check for GPU availability\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available and will be used for training!\")\n",
        "else:\n",
        "    print(\"GPU is not available. Training will use CPU.\")\n",
        "\n",
        "# Load intents from intents.json\n",
        "with open('intents.json') as file:\n",
        "    intents = json.load(file)\n",
        "\n",
        "# Preprocess data\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '.', '/', '@']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(pattern)  # Tokenize the sentence\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# Remove stopwords and lemmatize words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [\n",
        "    lemmatizer.lemmatize(word.lower())\n",
        "    for word in words\n",
        "    if word not in ignore_letters and word.lower() not in stop_words\n",
        "]\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "# Save words and classes for inference\n",
        "with open('words.pkl', 'wb') as f:\n",
        "    pickle.dump(words, f)\n",
        "with open('classes.pkl', 'wb') as f:\n",
        "    pickle.dump(classes, f)\n",
        "\n",
        "# Prepare training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "    bag = []\n",
        "    word_patterns = document[0]\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words:\n",
        "        bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# Shuffle and convert training data to NumPy arrays\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile the model using SGD optimizer\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "print(\"Training the model...\")\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=40, batch_size=5, verbose=1)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('chatbot_model.keras', hist)\n",
        "print(\"Model saved as 'chatbot_model.keras'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load necessary files\n",
        "try:\n",
        "    with open('intents.json') as file:\n",
        "        intents = json.load(file)\n",
        "\n",
        "    with open('words.pkl', 'rb') as file:\n",
        "        words = pickle.load(file)\n",
        "\n",
        "    with open('classes.pkl', 'rb') as file:\n",
        "        classes = pickle.load(file)\n",
        "\n",
        "    model = load_model('chatbot_model.keras')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Preprocess input\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# Convert input to bag-of-words format\n",
        "def bow(sentence, words):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0] * len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "    return np.array(bag)\n",
        "\n",
        "# Predict the class of the input\n",
        "def predict_class(sentence):\n",
        "    bow_vector = bow(sentence, words)\n",
        "    try:\n",
        "        res = model.predict(np.array([bow_vector]))[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction error: {e}\")\n",
        "        return []\n",
        "    ERROR_THRESHOLD = 0.1\n",
        "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results]\n",
        "\n",
        "# Get response based on predicted class\n",
        "def get_response(intents_list, intents_json):\n",
        "    if not intents_list:\n",
        "        return \"I'm sorry, I couldn't understand. Please try again.\"\n",
        "    tag = intents_list[0]['intent']\n",
        "    for intent in intents_json['intents']:\n",
        "        if intent['tag'] == tag:\n",
        "            return random.choice(intent['responses'])\n",
        "    return \"I'm sorry, I couldn't find a response for that.\"\n",
        "\n",
        "# Run the chatbot\n",
        "def chatbot():\n",
        "    print(\"\"\" Chatbot is ready! Type 'exit' to stop.\n",
        "  Welcome to medical chatbot please tell me your symptoms \"\"\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        intents_prediction = predict_class(user_input)\n",
        "        if intents_prediction:\n",
        "            print(f\"Predicted intents: {intents_prediction}\")  # Debugging line\n",
        "        response = get_response(intents_prediction, intents)\n",
        "        print(f\"Bot: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()\n"
      ],
      "metadata": {
        "id": "YewBlCg70fGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d08865-779a-4d91-d3f4-ae931e494d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Chatbot is ready! Type 'exit' to stop.\n",
            "  Welcome to medical chatbot please tell me your symptoms \n",
            "You: hi\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
            "Predicted intents: [{'intent': 'greetings', 'probability': '0.5011247'}, {'intent': 'medical_greeting', 'probability': '0.41329113'}]\n",
            "Bot: Hello!\n",
            "You: i have sour throat and fever with headache\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Predicted intents: [{'intent': 'symptom_analysis', 'probability': '0.8709054'}, {'intent': 'remedy_request', 'probability': '0.11413503'}]\n",
            "Bot: It seems you may have a common cold or flu. For a headache, consider taking paracetamol, and for a sore throat, warm salt water gargles may help. Consult a doctor for a detailed check-up.\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}